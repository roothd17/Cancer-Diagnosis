{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cancer Diagnosis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('E:\\Datasets\\Cancer Diagnosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating_Mutations</td>\n",
       "      <td>1</td>\n",
       "      <td>cyclin dependent kinases cdks regulate variety...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>2</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "      <td>recent evidence demonstrated acquired uniparen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "      <td>oncogenic mutations monomeric casitas b lineag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  Class  \\\n",
       "0   0  FAM58A  Truncating_Mutations      1   \n",
       "1   1     CBL                 W802*      2   \n",
       "2   2     CBL                 Q249E      2   \n",
       "3   3     CBL                 N454D      3   \n",
       "4   4     CBL                 L399V      4   \n",
       "\n",
       "                                                TEXT  \n",
       "0  cyclin dependent kinases cdks regulate variety...  \n",
       "1  abstract background non small cell lung cancer...  \n",
       "2  abstract background non small cell lung cancer...  \n",
       "3  recent evidence demonstrated acquired uniparen...  \n",
       "4  oncogenic mutations monomeric casitas b lineag...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data points :  3321\n",
      " Features :  ['ID' 'Gene' 'Variation']\n"
     ]
    }
   ],
   "source": [
    "print(' Data points : ', df.shape[0] )\n",
    "print(' Features : ', df.columns.values[0:3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAF4CAYAAAACBItaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nogcAAFAwih4AAEDBKHoAAAAF8/8A1zKpUSE8Yi0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x240c8d987f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Class.value_counts().plot(kind = 'bar', figsize = (15,6), color='violet' )\n",
    "plt.title(' Class Distribution ')\n",
    "plt.xlabel(' Class ')\n",
    "plt.ylabel(' Value Count ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    953\n",
       "4    686\n",
       "1    568\n",
       "2    452\n",
       "6    275\n",
       "5    242\n",
       "3     89\n",
       "9     37\n",
       "8     19\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 3> Classes 9, 8 and 3 have very few number of data points such type of classes are called <b>Sparse Classes</b> \n",
    "    \n",
    "Sparse Classes are those that have very few total observations. <br>\n",
    "    They can be problematic for certain machine learning algorithms, causing models to be overfit.  \n",
    "<br>\n",
    "All the rest Classes ( i,e; 1, 2, 4, 5, 6, 7 ) have > 240 observations ( data points )  and Class 3, 9, 8 have < 90 observations\n",
    "\n",
    "Even Class 3, 9, 8 combined have < 150 Observations\n",
    "\n",
    "So we combine these three ( Class 3, 9, 8 ) Class to a single Class ( say we name it Class 893 ) to gain better performence from our model\n",
    "\n",
    "And we use a seperate model later to Classify these three Classes independantly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Combining Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3321, 5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparse classes (in categorical features) are those that have very few total observations. \n",
    "#They can be problematic for certain machine learning algorithms, causing models to be overfit.\n",
    "\n",
    "data1 =  df.copy()\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newClass = []\n",
    "\n",
    "for c in data1.Class.values:\n",
    "    \n",
    "    if c == 8 or c == 9 or c == 3:\n",
    "        newClass.append(893)\n",
    "    else:\n",
    "        newClass.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1['newClass'] = newClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAGDCAYAAAB9UWKAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x240c8ddddd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1.newClass.value_counts().plot(kind = 'bar', figsize = (15,6), color='violet' )\n",
    "plt.title(' Class Distribution ')\n",
    "plt.xlabel(' Class ')\n",
    "plt.ylabel(' Value Count ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1.drop( \"Class\", axis = 1, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>newClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating_Mutations</td>\n",
       "      <td>cyclin dependent kinases cdks regulate variety...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  \\\n",
       "0   0  FAM58A  Truncating_Mutations   \n",
       "1   1     CBL                 W802*   \n",
       "\n",
       "                                                TEXT  newClass  \n",
       "0  cyclin dependent kinases cdks regulate variety...         1  \n",
       "1  abstract background non small cell lung cancer...         2  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y1 = data1['newClass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Train CV split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrd1, xted1, ytrd1, yted1 = train_test_split( data1, Y1 , stratify = Y1, test_size = 0.2 )\n",
    "\n",
    "xtrd1, xcvd1, ytrd1, ycvd1 = train_test_split( xtrd1, ytrd1, stratify = ytrd1, test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Matrix TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 226)\n",
      "(532, 226)\n",
      "(665, 226)\n",
      "(2124, 226)\n",
      "(532, 226)\n",
      "(665, 226)\n",
      "(2124, 1000)\n",
      "(532, 1000)\n",
      "(665, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Genes\n",
    "gTfVect1 = TfidfVectorizer( max_features = 1000 )\n",
    "\n",
    "xtrGeneTF1 = gTfVect1.fit_transform( xtrd1['Gene'] )\n",
    "xteGeneTF1 = gTfVect1.transform( xted1['Gene'] )\n",
    "xcvGeneTF1 = gTfVect1.transform( xcvd1['Gene'] )\n",
    "\n",
    "print(xtrGeneTF1.shape )\n",
    "print(xcvGeneTF1.shape )\n",
    "print(xteGeneTF1.shape )\n",
    "\n",
    "#variation\n",
    "vTfVect1 = TfidfVectorizer( max_features = 1000 )\n",
    "\n",
    "xtrVarTF1 = vTfVect1.fit_transform( xtrd1['Variation'] )\n",
    "xteVarTF1 = vTfVect1.transform( xted1['Variation'] )\n",
    "xcvVarTF1 = vTfVect1.transform( xcvd1['Variation'] )\n",
    "\n",
    "print(xtrGeneTF1.shape )\n",
    "print(xcvGeneTF1.shape )\n",
    "print(xteGeneTF1.shape )\n",
    "\n",
    "# Text\n",
    "teTfvect1 = TfidfVectorizer( max_features = 1000 )\n",
    "\n",
    "xtrTextTF1 = teTfvect1.fit_transform( xtrd1['TEXT'] )\n",
    "\n",
    "xtrTextTF1 = normalize( xtrTextTF1 , axis=0 )\n",
    "# Don't forget to normalize every feature\n",
    "\n",
    "xteTextTF1 = teTfvect1.fit_transform( xted1['TEXT'] )\n",
    "  # we use the same vectorizer that was trained on train data\n",
    "xteTextTF1 = normalize( xteTextTF1, axis=0 )\n",
    "\n",
    "xcvTextTF1 = teTfvect1.fit_transform( xcvd1['TEXT'] )\n",
    "xcvTextTF1 = normalize( xcvTextTF1, axis=0 ) # don't forget to normalize every feature\n",
    "\n",
    "print( xtrTextTF1.shape )\n",
    "print( xcvTextTF1.shape )\n",
    "print( xteTextTF1.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 2226)\n",
      "(665, 2226)\n",
      "(532, 2226)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "xtr2 = hstack(( xtrGeneTF1, xtrVarTF1, xtrTextTF1 )).tocsr()\n",
    "xte2 = hstack(( xteGeneTF1, xteVarTF1, xteTextTF1 )).tocsr()\n",
    "xcv2 = hstack(( xcvGeneTF1, xcvVarTF1, xcvTextTF1 )).tocsr()\n",
    "\n",
    "print( xtr2.shape )\n",
    "print( xte2.shape )\n",
    "print( xcv2.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  1e-06  the log loss is :  1.4687972991248666\n",
      "For values of alpha =  1e-05  the log loss is :  1.4566365751620225\n",
      "For values of alpha =  0.0001  the log loss is :  1.2799290960834055\n",
      "For values of alpha =  0.001  the log loss is :  1.2299620715710688\n",
      "For values of alpha =  0.01  the log loss is :  1.3477191301676261\n",
      "For values of alpha =  0.1  the log loss is :  1.6002053472026894\n",
      "For values of alpha =  1  the log loss is :  1.7184310549069408\n",
      "For values of alpha =  10  the log loss is :  1.730941204513949\n",
      "For values of alpha =  100  the log loss is :  1.7322881002777852\n"
     ]
    }
   ],
   "source": [
    "alpha = [ 10 ** x for x in range(-6, 3) ]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    classifier = SGDClassifier(class_weight='balanced', alpha = i, penalty='l2', loss='log', random_state=42)\n",
    "    classifier.fit(xtr2, ytrd1)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtr2, ytrd1)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcv2 )\n",
    "    \n",
    "    logError.append( log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of alpha = ', i, \" the log loss is : \",log_loss(ycvd1, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best alpha :  0.001\n",
      " The train log loss is :  0.7567304666109059\n",
      " The test log loss is :  1.0972456188008082\n",
      " The cv log loss is :  1.2299620715710688\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "bestA = alpha[ np.argmin(logError) ]\n",
    "\n",
    "clf = SGDClassifier(class_weight='balanced', alpha = bestA, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(xtr2, ytrd1)\n",
    "\n",
    "clf = CalibratedClassifierCV( clf, method=\"sigmoid\" )\n",
    "clf.fit(xtr2, ytrd1)\n",
    "\n",
    "print(' Best alpha : ', bestA)\n",
    "\n",
    "predictY = clf.predict_proba( xtr2 )\n",
    "print(' The train log loss is : ',log_loss( ytrd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xte2 )\n",
    "print(' The test log loss is : ',log_loss( yted1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xcv2 )\n",
    "print(' The cv log loss is : ',log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " % of points correctly classified :  61.65413533834586\n"
     ]
    }
   ],
   "source": [
    "y_ = clf.predict( xte2 )\n",
    "i =  y_ ^ yted1\n",
    "print(' % of points correctly classified : ', ( ( i.shape[0] - np.count_nonzero( i ) )  / i.shape[0] ) * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  1e-06  the log loss is :  1.458320838533109\n",
      "For values of alpha =  1e-05  the log loss is :  1.4671068426549754\n",
      "For values of alpha =  0.0001  the log loss is :  1.3787852521888202\n",
      "For values of alpha =  0.001  the log loss is :  1.3285852523862536\n",
      "For values of alpha =  0.01  the log loss is :  1.5195654038072075\n",
      "For values of alpha =  0.1  the log loss is :  1.6202019276963164\n",
      "For values of alpha =  1  the log loss is :  1.6839625575676258\n",
      "For values of alpha =  10  the log loss is :  1.683962565155213\n",
      "For values of alpha =  100  the log loss is :  1.6839626214672128\n"
     ]
    }
   ],
   "source": [
    "alpha = [ 10 ** x for x in range(-6, 3) ]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    classifier = SGDClassifier( alpha = i, penalty='l2', loss='hinge', random_state=42)\n",
    "    classifier.fit(xtr2, ytrd1)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtr2, ytrd1)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcv2 )\n",
    "    \n",
    "    logError.append( log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of alpha = ', i, \" the log loss is : \",log_loss(ycvd1, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best alpha :  0.001\n",
      " The train log loss is :  0.7231530990540695\n",
      " The test log loss is :  1.1877776963799551\n",
      " The cv log loss is :  1.3285852523862536\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "bestA = alpha[ np.argmin(logError) ]\n",
    "\n",
    "clf = SGDClassifier( alpha = bestA, penalty='l2', loss='hinge', random_state=42)\n",
    "clf.fit(xtr2, ytrd1)\n",
    "\n",
    "clf = CalibratedClassifierCV( clf, method=\"sigmoid\" )\n",
    "clf.fit(xtr2, ytrd1)\n",
    "\n",
    "print(' Best alpha : ', bestA)\n",
    "\n",
    "predictY = clf.predict_proba( xtr2 )\n",
    "print(' The train log loss is : ',log_loss( ytrd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xte2 )\n",
    "print(' The test log loss is : ',log_loss( yted1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xcv2 )\n",
    "print(' The cv log loss is : ',log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " % of points correctly classified :  61.65413533834586\n"
     ]
    }
   ],
   "source": [
    "y_ = clf.predict( xte2 )\n",
    "i =  y_ ^ yted1\n",
    "print(' % of points correctly classified : ', ( ( i.shape[0] - np.count_nonzero( i ) )  / i.shape[0] ) * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of K =  3  the log loss is :  1.309102314493684\n",
      "For values of K =  5  the log loss is :  1.2661574452774875\n",
      "For values of K =  6  the log loss is :  1.2574809203324075\n",
      "For values of K =  7  the log loss is :  1.2606531237743746\n",
      "For values of K =  8  the log loss is :  1.2638659913118246\n",
      "For values of K =  10  the log loss is :  1.2746841007536194\n",
      "For values of K =  15  the log loss is :  1.2808868245192049\n",
      "For values of K =  19  the log loss is :  1.2935249919765313\n",
      "For values of K =  21  the log loss is :  1.2928428777610008\n",
      "For values of K =  25  the log loss is :  1.2961464790722281\n",
      "For values of K =  31  the log loss is :  1.306381082212762\n",
      "For values of K =  41  the log loss is :  1.3264890372316778\n",
      "For values of K =  51  the log loss is :  1.3425767099519093\n",
      "For values of K =  61  the log loss is :  1.3504130998505512\n",
      "For values of K =  81  the log loss is :  1.3695687972941895\n",
      "For values of K =  91  the log loss is :  1.3793026850131156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = [ 3, 5, 6, 7, 8, 10, 15, 19, 21, 25, 31, 41, 51, 61, 81, 91] # Various value of Hyperparameter\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in k :\n",
    "    classifier = KNeighborsClassifier( n_neighbors = i ) \n",
    "    classifier.fit(xtr2, ytrd1)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtr2, ytrd1)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcv2 )\n",
    "    \n",
    "    logError.append( log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of K = ', i, \" the log loss is : \",log_loss(ycvd1, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Function for Response Coding of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feaDic( alpha, feature, df ):\n",
    "    \n",
    "    count = xtrd1[ feature ].value_counts()\n",
    "    # output:\n",
    "    #        {BRCA1      174\n",
    "    #         TP53       106\n",
    "    #         EGFR        86.....\n",
    "    \n",
    "    # featDict : Feature Dict, which contains the probability array for each gene/variation\n",
    "    featDict = dict()\n",
    "    \n",
    "    # denominator will contain the number of time that particular feature occured in whole data\n",
    "    \n",
    "    for i, denominator in count.items():\n",
    "        \n",
    "        # vec will contain ( P( yi == 1 / Gi ) probability of gene/variation belongs to particular class\n",
    "        # vec is 9 diamensional vector\n",
    "        vec = []\n",
    "        \n",
    "        c = [1,2,4,5,6,7,893]\n",
    "        \n",
    "        for j in c:\n",
    "           \n",
    "            # print( xtr.loc[ ( xtr['Class'] == 1 ) & ( xtr['Gene'] == 'BRCA1' )] )\n",
    "            #         ID   Gene             Variation  Class  \n",
    "            # 2470  2470  BRCA1                S1715C      1   \n",
    "            # 2486  2486  BRCA1                S1841R      1   \n",
    "            # 2614  2614  BRCA1                   M1R      1....\n",
    "            \n",
    "            # cls_cnt.shape[0] will return the number of rows\n",
    "\n",
    "            cls_cnt = xtrd1.loc[ ( xtrd1['newClass'] == j ) & ( xtrd1[feature] == i ) ]\n",
    "            \n",
    "            # cls_cnt.shape[0] will contain the number of time that particular feature occured in the whole data\n",
    "            \n",
    "            vec.append( ( cls_cnt.shape[0] + alpha * 10 ) / ( denominator + 90 * alpha ) )\n",
    "\n",
    "        # we are adding the gene/variation to the dict as key and vec as value\n",
    "        featDict[i] = vec\n",
    "    return featDict\n",
    "\n",
    "# when we caculate the probability of a feature belongs to any particular class, we apply laplace smoothing\n",
    "# (numerator + 10 *alpha) / (denominator + 90 *alpha ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def feature( alpha, feature, df ):\n",
    "   \n",
    "    featureDict = feaDic( alpha, feature, df )\n",
    "\n",
    "    count = xtrd1[feature].value_counts()\n",
    "    \n",
    "    # feat : Gene_variation feature, it will contain the feature for each feature value in the data\n",
    "    feat = []\n",
    "    \n",
    "    # for every feature values in the given data frame we will check if it is there in the train data then we will add...\n",
    "    #... the feature to gv_fea. if not we will add [1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9] to gv_fea\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        if row[feature] in dict( count ).keys():\n",
    "            feat.append( featureDict[ row[feature] ] )\n",
    "        else:\n",
    "            feat.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n",
    "\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 7)\n",
      "(532, 7)\n",
      "(665, 7)\n"
     ]
    }
   ],
   "source": [
    "# Response-coding of the Gene feature\n",
    "alpha = 1 # alpha is used for laplace smoothing\n",
    "\n",
    "xtrGeneRC = np.array( feature( alpha, \"Gene\", xtrd1) )\n",
    "xteGeneRC = np.array( feature( alpha, \"Gene\", xted1) )\n",
    "xcvGeneRC = np.array( feature( alpha, \"Gene\", xcvd1) )\n",
    "\n",
    "print(xtrGeneRC.shape )\n",
    "print(xcvGeneRC.shape )\n",
    "print(xteGeneRC.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 7)\n",
      "(532, 7)\n",
      "(665, 7)\n"
     ]
    }
   ],
   "source": [
    "# Response-coding of the Variation feature\n",
    "alpha = 1 # alpha is used for laplace smoothing\n",
    "\n",
    "xtrVarRC = np.array( feature( alpha, \"Variation\", xtrd1) )\n",
    "xteVarRC = np.array( feature( alpha, \"Variation\", xted1) )\n",
    "xcvVarRC = np.array( feature( alpha, \"Variation\", xcvd1) )\n",
    "\n",
    "print(xtrVarRC.shape )\n",
    "print(xcvVarRC.shape )\n",
    "print(xteVarRC.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cls_text is a data frame\n",
    "# 1. for every row in data fram consider the 'TEXT'\n",
    "# 2.     Split the words by space\n",
    "# 3.     Make a dict with those words\n",
    "# 4.     Increment its count whenever we see that word\n",
    "\n",
    "def extract(classText):\n",
    "    \n",
    "    dictionary = defaultdict( int )\n",
    "    \n",
    "    for index, row in classText.iterrows():\n",
    "        for word in row['TEXT'].split():\n",
    "            dictionary[word] +=1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def textResponseC(df):\n",
    "    \n",
    "    textFeat = np.zeros( ( df.shape[0], 7 ) )\n",
    "    \n",
    "    #c = [1,2,4,5,6,7,893]\n",
    "\n",
    "    for i in range(0,7):\n",
    "        row_index = 0\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            prob = 0\n",
    "            for word in row['TEXT'].split():\n",
    "                prob += math.log(( ( dictli[i].get( word, 0 ) + 10 ) / ( totaldict.get( word, 0 ) + 90 ) ))\n",
    "                # here 10 and 90 are alpha fro Laplace smoothing \n",
    "            \n",
    "            textFeat[ row_index ][i] = math.exp( prob / len( row['TEXT'].split() ))\n",
    "            row_index += 1\n",
    "        \n",
    "    return textFeat\n",
    "\n",
    "# Ref : #https://stackoverflow.com/a/1602964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictli = [] # dictli : contains 9 dictoinaries each corresponds to a class\n",
    "\n",
    "c = [1,2,4,5,6,7,893]\n",
    "\n",
    "for i in c:\n",
    "    classText = xtrd1[ xtrd1['newClass'] == i ]  # build a word dict based on the words in that class\n",
    "    dictli.append( extract( classText ) ) # append it to dictli\n",
    "\n",
    "# dictli[i] is build on i'th  class text data\n",
    "\n",
    "totaldict = extract( xtrd1 ) # totaldict is buid on whole training text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 7)\n",
      "(665, 7)\n",
      "(532, 7)\n"
     ]
    }
   ],
   "source": [
    "totaldict = extract( xtrd1 ) # totaldict is buid on whole training text data\n",
    "# Response coding of text features\n",
    "\n",
    "xtrTextRC  = textResponseC( xtrd1 )\n",
    "xteTextRC  = textResponseC( xted1 )\n",
    "xcvTextRC  = textResponseC( xcvd1 )\n",
    "\n",
    "print( xtrTextRC.shape ) \n",
    "print( xteTextRC.shape ) \n",
    "print( xcvTextRC.shape ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We convert each row values such that they sum to 1  \n",
    "\n",
    "xtrTextRC = ( xtrTextRC.T / xtrTextRC.sum( axis=1 ) ).T\n",
    "xteTextRC = ( xteTextRC.T / xteTextRC.sum( axis=1 ) ).T\n",
    "xcvTextRC = ( xcvTextRC.T / xcvTextRC.sum( axis=1 ) ).T\n",
    "\n",
    "# Ref : # https://stackoverflow.com/a/16202486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 21)\n",
      "(665, 21)\n",
      "(532, 21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xtrRC = np.hstack(( xtrGeneRC, xtrVarRC, xtrTextRC )) # giving error on just hstack() and not on np.hstack()\n",
    "xteRC = np.hstack(( xteGeneRC, xteVarRC, xteTextRC ))\n",
    "xcvRC = np.hstack(( xcvGeneRC, xcvVarRC, xcvTextRC ))  \n",
    "\n",
    "print( xtrRC.shape )\n",
    "print( xteRC.shape )\n",
    "print( xcvRC.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Response Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  1e-06  the log loss is :  1.2097329767518612\n",
      "For values of alpha =  1e-05  the log loss is :  1.1991886238924199\n",
      "For values of alpha =  0.0001  the log loss is :  1.1871013330642404\n",
      "For values of alpha =  0.001  the log loss is :  1.2306775662446807\n",
      "For values of alpha =  0.01  the log loss is :  1.2716702604378192\n",
      "For values of alpha =  0.1  the log loss is :  1.2833499267625812\n",
      "For values of alpha =  1  the log loss is :  1.2988574185974213\n",
      "For values of alpha =  10  the log loss is :  1.4329001028086221\n",
      "For values of alpha =  100  the log loss is :  1.4882458591820107\n"
     ]
    }
   ],
   "source": [
    "alpha = [ 10 ** x for x in range(-6, 3) ]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    classifier = SGDClassifier(class_weight='balanced', alpha = i, penalty='l2', loss='log', random_state=42)\n",
    "    classifier.fit(xtrRC, ytrd1)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtrRC, ytrd1)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcvRC )\n",
    "    \n",
    "    logError.append( log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of alpha = ', i, \" the log loss is : \",log_loss(ycvd1, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best alpha :  0.0001\n",
      " The train log loss is :  0.847346054797082\n",
      " The test log loss is :  1.0578775240871088\n",
      " The cv log loss is :  1.1871013330642404\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "bestA = alpha[ np.argmin(logError) ]\n",
    "\n",
    "clf = SGDClassifier(class_weight='balanced', alpha = bestA, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(xtrRC, ytrd1)\n",
    "\n",
    "clf = CalibratedClassifierCV( clf, method=\"sigmoid\" )\n",
    "clf.fit(xtrRC, ytrd1)\n",
    "\n",
    "print(' Best alpha : ', bestA)\n",
    "\n",
    "predictY = clf.predict_proba( xtrRC )\n",
    "print(' The train log loss is : ',log_loss( ytrd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xteRC )\n",
    "print(' The test log loss is : ',log_loss( yted1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xcvRC )\n",
    "print(' The cv log loss is : ',log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN  Response Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of K =  3  the log loss is :  1.1496646063660707\n",
      "For values of K =  5  the log loss is :  1.1271072704794478\n",
      "For values of K =  6  the log loss is :  1.124875237202507\n",
      "For values of K =  7  the log loss is :  1.1298541755794924\n",
      "For values of K =  8  the log loss is :  1.1228641944883615\n",
      "For values of K =  10  the log loss is :  1.1318325684713941\n",
      "For values of K =  15  the log loss is :  1.1561685601388505\n",
      "For values of K =  19  the log loss is :  1.170575958728822\n",
      "For values of K =  21  the log loss is :  1.1782535432088335\n",
      "For values of K =  25  the log loss is :  1.1839933122361526\n",
      "For values of K =  31  the log loss is :  1.195787700430199\n",
      "For values of K =  41  the log loss is :  1.206799952666955\n",
      "For values of K =  51  the log loss is :  1.2117733390622996\n",
      "For values of K =  61  the log loss is :  1.2167668204386415\n",
      "For values of K =  81  the log loss is :  1.2332846602789427\n",
      "For values of K =  91  the log loss is :  1.224721744635219\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = [ 3, 5, 6, 7, 8, 10, 15, 19, 21, 25, 31, 41, 51, 61, 81, 91] # Various value of Hyperparameter\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in k :\n",
    "    classifier = KNeighborsClassifier( n_neighbors = i ) \n",
    "    classifier.fit(xtrRC, ytrd1)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtrRC, ytrd1)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcvRC )\n",
    "    \n",
    "    logError.append( log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of K = ', i, \" the log loss is : \",log_loss(ycvd1, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best K :  8\n",
      " The train log loss is :  0.5460165182858353\n",
      " The test log loss is :  1.0314135355026817\n",
      " The cv log loss is :  1.1228641944883615\n"
     ]
    }
   ],
   "source": [
    "bestk = k[np.argmin( logError )]\n",
    "\n",
    "clf = KNeighborsClassifier( n_neighbors = bestk )\n",
    "clf.fit(xtrRC, ytrd1)\n",
    "\n",
    "clf = CalibratedClassifierCV( clf, method=\"sigmoid\" )\n",
    "clf.fit(xtrRC, ytrd1)\n",
    "\n",
    "print(' Best K : ', bestk)\n",
    "\n",
    "predictY = clf.predict_proba( xtrRC )\n",
    "print(' The train log loss is : ',log_loss( ytrd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xteRC )\n",
    "print(' The test log loss is : ',log_loss( yted1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xcvRC )\n",
    "print(' The cv log loss is : ',log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " % of points correctly classified :  60.902255639097746\n"
     ]
    }
   ],
   "source": [
    "y_ = clf.predict( xcvRC )\n",
    "i =  y_ ^ ycvd1\n",
    "print(' % of points correctly classified : ', ( ( i.shape[0] - np.count_nonzero( i ) )  / i.shape[0] ) * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  100  and depth =  2  the log loss is :  1.8135106327650334\n",
      "For values of alpha =  100  and depth =  5  the log loss is :  1.4653435642351722\n",
      "For values of alpha =  100  and depth =  7  the log loss is :  1.5518833462740955\n",
      "For values of alpha =  100  and depth =  10  the log loss is :  1.8170193084122674\n",
      "For values of alpha =  200  and depth =  2  the log loss is :  1.8584374638726346\n",
      "For values of alpha =  200  and depth =  5  the log loss is :  1.3582053710797792\n",
      "For values of alpha =  200  and depth =  7  the log loss is :  1.4448862491701397\n",
      "For values of alpha =  200  and depth =  10  the log loss is :  1.7739596446510717\n",
      "For values of alpha =  300  and depth =  2  the log loss is :  1.6753668927828127\n",
      "For values of alpha =  300  and depth =  5  the log loss is :  1.3287588166618869\n",
      "For values of alpha =  300  and depth =  7  the log loss is :  1.416561059059895\n",
      "For values of alpha =  300  and depth =  10  the log loss is :  1.7484154939873404\n",
      "For values of alpha =  500  and depth =  2  the log loss is :  1.8231509845333473\n",
      "For values of alpha =  500  and depth =  5  the log loss is :  1.327916658232618\n",
      "For values of alpha =  500  and depth =  7  the log loss is :  1.4342177599855181\n",
      "For values of alpha =  500  and depth =  10  the log loss is :  1.7532803171194906\n",
      "For values of alpha =  1000  and depth =  2  the log loss is :  1.670741882974861\n",
      "For values of alpha =  1000  and depth =  5  the log loss is :  1.3795847176007232\n",
      "For values of alpha =  1000  and depth =  7  the log loss is :  1.4717070568993684\n",
      "For values of alpha =  1000  and depth =  10  the log loss is :  1.7623378566698389\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "alpha = [100,200,300, 500,1000] # Various value of Hyperparameter\n",
    "max_depth = [2, 5, 7, 10]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    for j in max_depth:\n",
    "        classifier = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n",
    "        classifier.fit(xtrRC, ytrd1)\n",
    "\n",
    "        # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "        # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "        # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "\n",
    "        clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "\n",
    "        # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "        # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "\n",
    "        clf.fit(xtrRC, ytrd1)\n",
    "\n",
    "        predictY = clf.predict_proba( xcvRC )\n",
    "\n",
    "        logError.append( log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "        print('For values of alpha = ', i,' and depth = ', j, \" the log loss is : \",log_loss(ycvd1, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  100  and depth =  5  the log loss is :  1.8286017630445017\n",
      "For values of alpha =  100  and depth =  10  the log loss is :  1.8095730920310142\n",
      "For values of alpha =  100  and depth =  20  the log loss is :  1.7970179946986695\n",
      "For values of alpha =  200  and depth =  5  the log loss is :  1.8133310093991468\n",
      "For values of alpha =  200  and depth =  10  the log loss is :  1.7959032569314377\n",
      "For values of alpha =  200  and depth =  20  the log loss is :  1.7875699946931947\n",
      "For values of alpha =  500  and depth =  5  the log loss is :  1.7955359030590359\n",
      "For values of alpha =  500  and depth =  10  the log loss is :  1.7885713206960039\n",
      "For values of alpha =  500  and depth =  20  the log loss is :  1.782934556969202\n",
      "For values of alpha =  1000  and depth =  5  the log loss is :  1.7958811959984269\n",
      "For values of alpha =  1000  and depth =  10  the log loss is :  1.7904618647087542\n",
      "For values of alpha =  1000  and depth =  20  the log loss is :  1.7854094176380635\n",
      "For values of alpha =  2000  and depth =  5  the log loss is :  1.7899986769176195\n",
      "For values of alpha =  2000  and depth =  10  the log loss is :  1.7904137314578212\n",
      "For values of alpha =  2000  and depth =  20  the log loss is :  1.7852703737447695\n"
     ]
    }
   ],
   "source": [
    "alpha = [100,200,500,1000,2000] # Various value of Hyperparameter\n",
    "max_depth = [5, 10, 20]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    for j in max_depth:\n",
    "        classifier = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n",
    "        classifier.fit(xtr2, ytrd1)\n",
    "\n",
    "        # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "        # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "        # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "\n",
    "        clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "\n",
    "        # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "        # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "\n",
    "        clf.fit(xtr2, ytrd1)\n",
    "\n",
    "        predictY = clf.predict_proba( xcv2 )\n",
    "\n",
    "        logError.append( log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "        print('For values of alpha = ', i,' and depth = ', j, \" the log loss is : \",log_loss(ycvd1, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## XG Boost Response Coding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  0.01  and depth =  1  the log loss is :  3.959960525623231\n",
      "For values of alpha =  0.01  and depth =  5  the log loss is :  2.540907705410178\n",
      "For values of alpha =  0.01  and depth =  10  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0.01  and depth =  50  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0.01  and depth =  100  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0.01  and depth =  500  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0.1  and depth =  1  the log loss is :  3.959960525623231\n",
      "For values of alpha =  0.1  and depth =  5  the log loss is :  2.540907705410178\n",
      "For values of alpha =  0.1  and depth =  10  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0.1  and depth =  50  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0.1  and depth =  100  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0.1  and depth =  500  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0  and depth =  1  the log loss is :  3.959960525623231\n",
      "For values of alpha =  0  and depth =  5  the log loss is :  2.540907705410178\n",
      "For values of alpha =  0  and depth =  10  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0  and depth =  50  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0  and depth =  100  the log loss is :  2.535579048226781\n",
      "For values of alpha =  0  and depth =  500  the log loss is :  2.535579048226781\n",
      "For values of alpha =  1  and depth =  1  the log loss is :  3.959960525623231\n",
      "For values of alpha =  1  and depth =  5  the log loss is :  2.540907705410178\n",
      "For values of alpha =  1  and depth =  10  the log loss is :  2.535579048226781\n",
      "For values of alpha =  1  and depth =  50  the log loss is :  2.535579048226781\n",
      "For values of alpha =  1  and depth =  100  the log loss is :  2.535579048226781\n",
      "For values of alpha =  1  and depth =  500  the log loss is :  2.535579048226781\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "max_depth = [ 1, 5, 10, 50, 100, 500 ]\n",
    "learning_rate = [ 0.01, 0.1, 0 , 1  ]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in learning_rate :\n",
    "    for j in max_depth:\n",
    "        \n",
    "        classifier = xgb.XGBClassifier( objective = \"binary:logistic\", random_state = 42, max_depth = j, eta = i )\n",
    "        classifier.fit(xtrRC, ytrd1)\n",
    "\n",
    "        # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "        # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "        # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "\n",
    "        clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "\n",
    "        # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "        # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "\n",
    "        clf.fit(xtrRC, ytrd1)\n",
    "\n",
    "        predictY = clf.predict_proba( xcvRC )\n",
    "\n",
    "        logError.append( log_loss( ycvd1, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "        print('For values of alpha = ', i,' and depth = ', j, \" the log loss is : \",log_loss(ycvd1, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------------+------------------+---------------+-----------------+\n",
      "|             Model             |  Featurization   |  Train log loss  |  CV log loss  |  Test log loss  |\n",
      "+-------------------------------+------------------+------------------+---------------+-----------------+\n",
      "|      Logistic Regression      |      TFIDF       |      0.7567      |     1.2299    |      1.0972     |\n",
      "|  Linear SVM (balanced class)  |      TFIDF       |      0.7231      |     1.3285    |      1.1877     |\n",
      "|      K Nearest Neighbour      |      TFIDF       |        *         |     1.2574    |        *        |\n",
      "|         Random Forest         |      TFIDF       |        *         |     1.7852    |        *        |\n",
      "|      Logistic Regression      |  ResponseCoding  |      0.8473      |     1.1871    |      1.0578     |\n",
      "|      K Nearest Neighbour      |  ResponseCoding  |      0.5460      |     1.1228    |      1.0314     |\n",
      "|         Random Forest         |  ResponseCoding  |        *         |     1.3279    |        *        |\n",
      "|            XG Boost           |  ResponseCoding  |        *         |     2.5355    |        *        |\n",
      "+-------------------------------+------------------+------------------+---------------+-----------------+\n",
      "\n",
      "\t\t\t * : ignored calculation since CV log loss > 1.2 \n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "    \n",
    "x = PrettyTable()\n",
    "   \n",
    "x.field_names = [ \" Model \", \" Featurization \", \" Train log loss \", \" CV log loss \", \" Test log loss \" ]\n",
    "\n",
    "x.add_row( [ \" Logistic Regression \", \" TFIDF \", \" 0.7567 \", \" 1.2299 \", \" 1.0972 \" ] )\n",
    "x.add_row( [ \" Linear SVM (balanced class) \", \" TFIDF \", \" 0.7231 \", \" 1.3285 \", \" 1.1877 \" ] )\n",
    "x.add_row( [ \" K Nearest Neighbour \", \" TFIDF \", \" * \", \" 1.2574 \", \" * \" ] )\n",
    "x.add_row( [ \" Random Forest \", \" TFIDF \", \" * \", \" 1.7852 \", \" * \" ] )\n",
    "\n",
    "x.add_row( [ \" Logistic Regression \", \" ResponseCoding \", \" 0.8473 \", \" 1.1871 \", \" 1.0578 \" ] )\n",
    "x.add_row( [ \" K Nearest Neighbour \", \" ResponseCoding \", \" 0.5460 \", \" 1.1228 \", \" 1.0314 \" ] )\n",
    "x.add_row( [ \" Random Forest \", \" ResponseCoding \", \" * \", \" 1.3279 \", \" * \" ] )\n",
    "x.add_row( [ \" XG Boost \", \" ResponseCoding \", \" * \", \" 2.5355 \", \" * \" ] )\n",
    "\n",
    "\n",
    "print( x )\n",
    "print(\"\\n\\t\\t\\t * : ignored calculation since CV log loss > 1.2 \")\n",
    "# Ref : http://zetcode.com/python/prettytable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the log loss is not still not quite low ( < 1.0 ) we need to perform futher Feature Engineering <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating new features + combining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3321, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 =  data1.copy()\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>newClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating_Mutations</td>\n",
       "      <td>cyclin dependent kinases cdks regulate variety...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  \\\n",
       "0   0  FAM58A  Truncating_Mutations   \n",
       "1   1     CBL                 W802*   \n",
       "\n",
       "                                                TEXT  newClass  \n",
       "0  cyclin dependent kinases cdks regulate variety...         1  \n",
       "1  abstract background non small cell lung cancer...         2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene = dict( data2.Gene.value_counts() )\n",
    "len( gene )\n",
    "gene['CBL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop for calculating len of each Gene feature and no. of time it appears in the dataset\n",
    "\n",
    "li = []\n",
    "le = []\n",
    "for ge in data2.Gene:\n",
    "    li.append( gene[ ge ] )\n",
    "    le.append( len(ge) )\n",
    "data2['countGene'] = li\n",
    "data2['lenGene'] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>newClass</th>\n",
       "      <th>countGene</th>\n",
       "      <th>lenGene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating_Mutations</td>\n",
       "      <td>cyclin dependent kinases cdks regulate variety...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>recent evidence demonstrated acquired uniparen...</td>\n",
       "      <td>893</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>oncogenic mutations monomeric casitas b lineag...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  \\\n",
       "0   0  FAM58A  Truncating_Mutations   \n",
       "1   1     CBL                 W802*   \n",
       "2   2     CBL                 Q249E   \n",
       "3   3     CBL                 N454D   \n",
       "4   4     CBL                 L399V   \n",
       "\n",
       "                                                TEXT  newClass  countGene  \\\n",
       "0  cyclin dependent kinases cdks regulate variety...         1          1   \n",
       "1  abstract background non small cell lung cancer...         2         25   \n",
       "2  abstract background non small cell lung cancer...         2         25   \n",
       "3  recent evidence demonstrated acquired uniparen...       893         25   \n",
       "4  oncogenic mutations monomeric casitas b lineag...         4         25   \n",
       "\n",
       "   lenGene  \n",
       "0        6  \n",
       "1        3  \n",
       "2        3  \n",
       "3        3  \n",
       "4        3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = dict( data2.Variation.value_counts() )\n",
    "len(var)\n",
    "var['Truncating_Mutations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop for calculating len of each Variation feature and no. of time it appears in the dataset\n",
    "\n",
    "li = []\n",
    "le = []\n",
    "for v in data2.Variation:\n",
    "    li.append( var[ v ] )\n",
    "    le.append( len(v) )\n",
    "data2['countVar'] = li\n",
    "data2['lenVar'] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>newClass</th>\n",
       "      <th>countGene</th>\n",
       "      <th>lenGene</th>\n",
       "      <th>countVar</th>\n",
       "      <th>lenVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating_Mutations</td>\n",
       "      <td>cyclin dependent kinases cdks regulate variety...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>93</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>recent evidence demonstrated acquired uniparen...</td>\n",
       "      <td>893</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>oncogenic mutations monomeric casitas b lineag...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  \\\n",
       "0   0  FAM58A  Truncating_Mutations   \n",
       "1   1     CBL                 W802*   \n",
       "2   2     CBL                 Q249E   \n",
       "3   3     CBL                 N454D   \n",
       "4   4     CBL                 L399V   \n",
       "\n",
       "                                                TEXT  newClass  countGene  \\\n",
       "0  cyclin dependent kinases cdks regulate variety...         1          1   \n",
       "1  abstract background non small cell lung cancer...         2         25   \n",
       "2  abstract background non small cell lung cancer...         2         25   \n",
       "3  recent evidence demonstrated acquired uniparen...       893         25   \n",
       "4  oncogenic mutations monomeric casitas b lineag...         4         25   \n",
       "\n",
       "   lenGene  countVar  lenVar  \n",
       "0        6        93      20  \n",
       "1        3         1       5  \n",
       "2        3         1       5  \n",
       "3        3         1       5  \n",
       "4        3         1       5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = data2['TEXT']\n",
    "a = list(a)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mode\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word = []\n",
    "max = []\n",
    "for sent in tqdm( a ) :\n",
    "    for w in sent.split():\n",
    "          word.append(w)\n",
    "    mc = Counter(word)\n",
    "    max.append(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 3321/3321 [00:00<00:00, 1110788.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop for calculating len of each TEXT features\n",
    "le = []\n",
    "for sent in tqdm( a ):\n",
    "    le.append( len(sent) )\n",
    "\n",
    "data2['lenText'] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>newClass</th>\n",
       "      <th>countGene</th>\n",
       "      <th>lenGene</th>\n",
       "      <th>countVar</th>\n",
       "      <th>lenVar</th>\n",
       "      <th>lenText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating_Mutations</td>\n",
       "      <td>cyclin dependent kinases cdks regulate variety...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>93</td>\n",
       "      <td>20</td>\n",
       "      <td>30836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>27844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>27844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>recent evidence demonstrated acquired uniparen...</td>\n",
       "      <td>893</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>28093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>oncogenic mutations monomeric casitas b lineag...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  \\\n",
       "0   0  FAM58A  Truncating_Mutations   \n",
       "1   1     CBL                 W802*   \n",
       "2   2     CBL                 Q249E   \n",
       "3   3     CBL                 N454D   \n",
       "4   4     CBL                 L399V   \n",
       "\n",
       "                                                TEXT  newClass  countGene  \\\n",
       "0  cyclin dependent kinases cdks regulate variety...         1          1   \n",
       "1  abstract background non small cell lung cancer...         2         25   \n",
       "2  abstract background non small cell lung cancer...         2         25   \n",
       "3  recent evidence demonstrated acquired uniparen...       893         25   \n",
       "4  oncogenic mutations monomeric casitas b lineag...         4         25   \n",
       "\n",
       "   lenGene  countVar  lenVar  lenText  \n",
       "0        6        93      20    30836  \n",
       "1        3         1       5    27844  \n",
       "2        3         1       5    27844  \n",
       "3        3         1       5    28093  \n",
       "4        3         1       5    31649  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32995    53\n",
       "37392    47\n",
       "34377    42\n",
       "26199    31\n",
       "27133    31\n",
       "Name: lenText, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( len( data2.lenText.value_counts() ),' Unique length of TEXT feature' )\n",
    "\n",
    "data2.lenText.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4    1276\n",
       "5    1073\n",
       "6     500\n",
       "3     420\n",
       "7      31\n",
       "2      20\n",
       "8       1\n",
       "Name: lenGene, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( len( data2.lenGene.value_counts() ),' Unique length of Gene feature' )\n",
    "\n",
    "data2.lenGene.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5     1647\n",
       "6      569\n",
       "4      434\n",
       "20     104\n",
       "8       88\n",
       "Name: lenVar, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( len( data2.lenVar.value_counts() ),' Unique length of Variance feature' )\n",
    "\n",
    "data2.lenVar.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnAAAAJGCAYAAADbMLgTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x240bf586588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.FacetGrid(data2, hue='newClass', size=8).map(sns.distplot, \"lenGene\").add_legend()\n",
    "plt.title('Univariate Analysis of lenGene')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3321/3321 [00:03<00:00, 918.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop for calculating 1st word and last word of every TEXT feature\n",
    "li = []\n",
    "w1 = [] # 1st word\n",
    "wl = [] # last word\n",
    "\n",
    "for sent in tqdm(a):\n",
    "    for w in sent.split():\n",
    "        li.append(w)\n",
    "    \n",
    "    w1.append( li[0] )\n",
    "    wl.append( li[-1] )\n",
    "    li = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2['text1st'] = w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2['textLast'] = wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>newClass</th>\n",
       "      <th>countGene</th>\n",
       "      <th>lenGene</th>\n",
       "      <th>countVar</th>\n",
       "      <th>lenVar</th>\n",
       "      <th>lenText</th>\n",
       "      <th>text1st</th>\n",
       "      <th>textLast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating_Mutations</td>\n",
       "      <td>cyclin dependent kinases cdks regulate variety...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>93</td>\n",
       "      <td>20</td>\n",
       "      <td>30836</td>\n",
       "      <td>cyclin</td>\n",
       "      <td>females</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>27844</td>\n",
       "      <td>abstract</td>\n",
       "      <td>tumorigenesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>27844</td>\n",
       "      <td>abstract</td>\n",
       "      <td>tumorigenesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>recent evidence demonstrated acquired uniparen...</td>\n",
       "      <td>893</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>28093</td>\n",
       "      <td>recent</td>\n",
       "      <td>attractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>oncogenic mutations monomeric casitas b lineag...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31649</td>\n",
       "      <td>oncogenic</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  \\\n",
       "0   0  FAM58A  Truncating_Mutations   \n",
       "1   1     CBL                 W802*   \n",
       "2   2     CBL                 Q249E   \n",
       "3   3     CBL                 N454D   \n",
       "4   4     CBL                 L399V   \n",
       "\n",
       "                                                TEXT  newClass  countGene  \\\n",
       "0  cyclin dependent kinases cdks regulate variety...         1          1   \n",
       "1  abstract background non small cell lung cancer...         2         25   \n",
       "2  abstract background non small cell lung cancer...         2         25   \n",
       "3  recent evidence demonstrated acquired uniparen...       893         25   \n",
       "4  oncogenic mutations monomeric casitas b lineag...         4         25   \n",
       "\n",
       "   lenGene  countVar  lenVar  lenText    text1st       textLast  \n",
       "0        6        93      20    30836     cyclin        females  \n",
       "1        3         1       5    27844   abstract  tumorigenesis  \n",
       "2        3         1       5    27844   abstract  tumorigenesis  \n",
       "3        3         1       5    28093     recent     attractive  \n",
       "4        3         1       5    31649  oncogenic         cancer  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Created :<br>\n",
    "    <b>countGene</b> : No. of times that Gene occour in the dataset <br>\n",
    "    <b>lenGene</b> : Length of that Gene<br>\n",
    "    <b>countVar</b> : No. of times that Variation occour in the dataset <br>\n",
    "    <b>lenVar</b> : Length of that Variation<br>\n",
    "    <b>text1st</b> : First word in the feature TEXT<br>\n",
    "    <b>textLast</b> :  Last word in the feature TEXT<br>\n",
    "    <b>lenText</b>  : Length of the text ( no. of words present ) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "abstract        311\n",
       "mutations       123\n",
       "introduction    113\n",
       "purpose          84\n",
       "tumor            74\n",
       "Name: text1st, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( len( data2.text1st.value_counts() ) )\n",
    "data2.text1st.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cancer        98\n",
       "mutations     92\n",
       "brca1         80\n",
       "patients      69\n",
       "inhibitors    65\n",
       "Name: textLast, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( len( data2.textLast.value_counts() ) )\n",
    "\n",
    "data2.textLast.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y2 = data2['newClass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train CV split and Creating Data Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Train CV split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrd2, xted2, ytrd2, yted2 = train_test_split( data2, Y2 , stratify = Y2, test_size = 0.2 )\n",
    "\n",
    "xtrd2, xcvd2, ytrd2, ycvd2 = train_test_split( xtrd2, ytrd2, stratify = ytrd2, test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  for TF IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 237)\n",
      "(532, 237)\n",
      "(665, 237)\n",
      "(2124, 237)\n",
      "(532, 237)\n",
      "(665, 237)\n",
      "(2124, 1000)\n",
      "(532, 1000)\n",
      "(665, 1000)\n",
      "(2124, 472)\n",
      "(665, 472)\n",
      "(532, 472)\n",
      "(2124, 561)\n",
      "(665, 561)\n",
      "(532, 561)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Genes\n",
    "gTfVect2 = TfidfVectorizer( max_features = 1000 )\n",
    "\n",
    "xtrGeneTF2 = gTfVect2.fit_transform( xtrd2['Gene'] )\n",
    "xteGeneTF2 = gTfVect2.transform( xted2['Gene'] )\n",
    "xcvGeneTF2 = gTfVect2.transform( xcvd2['Gene'] )\n",
    "\n",
    "print(xtrGeneTF2.shape )\n",
    "print(xcvGeneTF2.shape )\n",
    "print(xteGeneTF2.shape )\n",
    "\n",
    "#variation\n",
    "vTfVect2 = TfidfVectorizer( max_features = 1000 )\n",
    "\n",
    "xtrVarTF2 = vTfVect2.fit_transform( xtrd2['Variation'] )\n",
    "xteVarTF2 = vTfVect2.transform( xted2['Variation'] )\n",
    "xcvVarTF2 = vTfVect2.transform( xcvd2['Variation'] )\n",
    "\n",
    "print(xtrGeneTF2.shape )\n",
    "print(xcvGeneTF2.shape )\n",
    "print(xteGeneTF2.shape )\n",
    "\n",
    "# Text\n",
    "teTfvect2 = TfidfVectorizer( max_features = 1000 )\n",
    "\n",
    "xtrTextTF2 = teTfvect2.fit_transform( xtrd2['TEXT'] )\n",
    "xtrTextTF2 = normalize( xtrTextTF2 , axis=0 ) # Don't forget to normalize every feature\n",
    "\n",
    "xteTextTF2 = teTfvect2.transform( xted2['TEXT'] )\n",
    "xteTextTF2 = normalize( xteTextTF2, axis=0 )\n",
    "\n",
    "xcvTextTF2 = teTfvect2.transform( xcvd2['TEXT'] )\n",
    "xcvTextTF2 = normalize( xcvTextTF2, axis=0 ) \n",
    "\n",
    "print( xtrTextTF2.shape )\n",
    "print( xcvTextTF2.shape )\n",
    "print( xteTextTF2.shape )\n",
    "\n",
    "\n",
    "# First word in Text\n",
    "t1stTfVect2 = TfidfVectorizer( max_features = 1000 )\n",
    "\n",
    "xtrT1stTF2 = t1stTfVect2.fit_transform( xtrd2['text1st'] )\n",
    "xteT1stTF2 = t1stTfVect2.transform( xted2['text1st'] )\n",
    "xcvT1stTF2 = t1stTfVect2.transform( xcvd2['text1st'] )\n",
    "\n",
    "print(xtrT1stTF2.shape )\n",
    "print(xteT1stTF2.shape )\n",
    "print(xcvT1stTF2.shape )\n",
    "\n",
    "\n",
    "# Last word in Text\n",
    "tLTfVect2 = TfidfVectorizer( max_features = 1000 )\n",
    "\n",
    "xtrTLTF2 = tLTfVect2.fit_transform( xtrd2['textLast'] )\n",
    "xteTLTF2 = tLTfVect2.transform( xted2['textLast'] )\n",
    "xcvTLTF2 = tLTfVect2.transform( xcvd2['textLast'] )\n",
    "\n",
    "print(xtrTLTF2.shape )\n",
    "print(xteTLTF2.shape )\n",
    "print(xcvTLTF2.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 1)\n",
      "(665, 1)\n",
      "(532, 1)\n",
      "(2124, 1)\n",
      "(665, 1)\n",
      "(532, 1)\n",
      "(2124, 1)\n",
      "(665, 1)\n",
      "(532, 1)\n",
      "(2124, 1)\n",
      "(665, 1)\n",
      "(532, 1)\n",
      "(2124, 1)\n",
      "(665, 1)\n",
      "(532, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# countGene\n",
    "norm = Normalizer()\n",
    "\n",
    "xtrCG = norm.fit_transform( xtrd2['countGene'].values.reshape(-1,1) )\n",
    "xcvCG = norm.transform( xcvd2['countGene'].values.reshape(-1,1) )\n",
    "xteCG = norm.transform( xted2['countGene'].values.reshape(-1,1) )\n",
    "print(xtrCG.shape)\n",
    "print(xteCG.shape)\n",
    "print(xcvCG.shape)\n",
    "\n",
    "\n",
    "# countVar\n",
    "norm = Normalizer()\n",
    "\n",
    "xtrCV = norm.fit_transform( xtrd2['countVar'].values.reshape(-1,1) )\n",
    "xcvCV = norm.transform( xcvd2['countVar'].values.reshape(-1,1) )\n",
    "xteCV = norm.transform( xted2['countVar'].values.reshape(-1,1) )\n",
    "print(xtrCV.shape)\n",
    "print(xteCV.shape)\n",
    "print(xcvCV.shape)\n",
    "\n",
    "\n",
    "# lenVar\n",
    "norm = Normalizer()\n",
    "\n",
    "xtrLV = norm.fit_transform( xtrd2['lenVar'].values.reshape(-1,1) )\n",
    "xcvLV = norm.transform( xcvd2['lenVar'].values.reshape(-1,1) )\n",
    "xteLV = norm.transform( xted2['lenVar'].values.reshape(-1,1) )\n",
    "print(xtrLV.shape)\n",
    "print(xteLV.shape)\n",
    "print(xcvLV.shape)\n",
    "\n",
    "\n",
    "#lenGene\n",
    "norm = Normalizer()\n",
    "\n",
    "xtrLG = norm.fit_transform( xtrd2['lenGene'].values.reshape(-1,1) )\n",
    "xcvLG = norm.transform( xcvd2['lenGene'].values.reshape(-1,1) )\n",
    "xteLG = norm.transform( xted2['lenGene'].values.reshape(-1,1) )\n",
    "print(xtrLG.shape)\n",
    "print(xteLG.shape)\n",
    "print(xcvLG.shape)\n",
    "\n",
    "\n",
    "#lenText\n",
    "norm = Normalizer()\n",
    "\n",
    "xtrLT = norm.fit_transform( xtrd2['lenText'].values.reshape(-1,1) )\n",
    "xcvLT = norm.transform( xcvd2['lenText'].values.reshape(-1,1) )\n",
    "xteLT = norm.transform( xted2['lenText'].values.reshape(-1,1) )\n",
    "print(xtrLT.shape)\n",
    "print(xteLT.shape)\n",
    "print(xcvLT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 3275)\n",
      "(665, 3275)\n",
      "(532, 3275)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "xtr3 = hstack(( xtrGeneTF2, xtrVarTF2, xtrTextTF2, xtrT1stTF2, xtrTLTF2, xtrCG, xtrCV, xtrLV, xtrLG, xtrLT )).tocsr()\n",
    "xte3 = hstack(( xteGeneTF2, xteVarTF2, xteTextTF2, xteT1stTF2, xteTLTF2, xteCG, xteCV, xteLV, xteLG, xteLT )).tocsr()\n",
    "xcv3 = hstack(( xcvGeneTF2, xcvVarTF2, xcvTextTF2, xcvT1stTF2, xcvTLTF2, xcvCG, xcvCV, xcvLV, xcvLG, xcvLT )).tocsr()\n",
    "\n",
    "print( xtr3.shape )\n",
    "print( xte3.shape )\n",
    "print( xcv3.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings  # removes warning messages from the output\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  1e-06  the log loss is :  0.9862240536254596\n",
      "For values of alpha =  1e-05  the log loss is :  0.9892814497430283\n",
      "For values of alpha =  0.0001  the log loss is :  0.9768187905561545\n",
      "For values of alpha =  0.001  the log loss is :  0.9783214768246432\n",
      "For values of alpha =  0.01  the log loss is :  1.0876345920493318\n",
      "For values of alpha =  0.1  the log loss is :  1.1901507106049096\n",
      "For values of alpha =  1  the log loss is :  1.330142396876936\n",
      "For values of alpha =  10  the log loss is :  1.539050725671884\n",
      "For values of alpha =  100  the log loss is :  1.5957789173148795\n"
     ]
    }
   ],
   "source": [
    "alpha = [ 10 ** x for x in range(-6, 3) ]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    classifier = SGDClassifier( alpha = i, penalty='l2', loss='log', random_state = 42 )\n",
    "    classifier.fit(xtr3, ytrd2)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtr3, ytrd2)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcv3 )\n",
    "    \n",
    "    logError.append( log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of alpha = ', i, \" the log loss is : \",log_loss(ycvd2, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best alpha :  0.0001\n",
      " The train log loss is :  0.5302739124807866\n",
      " The test log loss is :  0.9257778414597118\n",
      " The cv log loss is :  0.9768187905561545\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "bestA = alpha[ np.argmin(logError) ]\n",
    "clf = SGDClassifier( alpha = bestA, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(xtr3, ytrd2)\n",
    "\n",
    "clf = CalibratedClassifierCV( clf, method=\"sigmoid\" )\n",
    "clf.fit(xtr3, ytrd2)\n",
    "\n",
    "print(' Best alpha : ', bestA)\n",
    "\n",
    "predictY = clf.predict_proba( xtr3 )\n",
    "print(' The train log loss is : ',log_loss( ytrd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xte3 )\n",
    "print(' The test log loss is : ',log_loss( yted2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xcv3 )\n",
    "print(' The cv log loss is : ',log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " % of points correctly classified :  66.9172932330827\n"
     ]
    }
   ],
   "source": [
    "y_ = clf.predict( xte3 )\n",
    "i =  y_ ^ yted2\n",
    "print(' % of points correctly classified : ', ( ( i.shape[0] - np.count_nonzero( i ) )  / i.shape[0] ) * 100 )\n",
    "\n",
    "print(' % of points missclassified  : ', ( np.count_nonzero(( clf.predict( xte3 )- yted2 )) / yted2.shape[0] )* 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  1e-06  the log loss is :  0.9872505633126057\n",
      "For values of alpha =  1e-05  the log loss is :  0.9993229913796619\n",
      "For values of alpha =  0.0001  the log loss is :  0.9945911488138485\n",
      "For values of alpha =  0.001  the log loss is :  0.9921881085261784\n",
      "For values of alpha =  0.01  the log loss is :  1.0648666101555004\n",
      "For values of alpha =  0.1  the log loss is :  1.120345617768938\n",
      "For values of alpha =  1  the log loss is :  1.330472839940044\n",
      "For values of alpha =  10  the log loss is :  1.5935044871200792\n",
      "For values of alpha =  100  the log loss is :  1.593501431762243\n"
     ]
    }
   ],
   "source": [
    "alpha = [ 10 ** x for x in range(-6, 3) ]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    classifier = SGDClassifier(class_weight = 'balanced', alpha = i, penalty='l2', loss='hinge', random_state = 42 )\n",
    "    classifier.fit(xtr3, ytrd2)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtr3, ytrd2)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcv3 )\n",
    "    \n",
    "    logError.append( log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of alpha = ', i, \" the log loss is : \",log_loss(ycvd2, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best alpha :  1e-06\n",
      " The train log loss is :  0.576004951297067\n",
      " The test log loss is :  0.9432775710318827\n",
      " The cv log loss is :  0.9946360345459967\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "bestA = alpha[ np.argmin(logError) ]\n",
    "clf = SGDClassifier(class_weight = 'balanced', alpha = bestA, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(xtr3, ytrd2)\n",
    "\n",
    "clf = CalibratedClassifierCV( clf, method=\"sigmoid\" )\n",
    "clf.fit(xtr3, ytrd2)\n",
    "\n",
    "print(' Best alpha : ', bestA)\n",
    "\n",
    "predictY = clf.predict_proba( xtr3 )\n",
    "print(' The train log loss is : ',log_loss( ytrd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xte3 )\n",
    "print(' The test log loss is : ',log_loss( yted2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xcv3 )\n",
    "print(' The cv log loss is : ',log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " % of points correctly classified :  68.1203007518797\n",
      " % of points missclassified  :  31.879699248120303\n"
     ]
    }
   ],
   "source": [
    "y_ = clf.predict( xte3 )\n",
    "i =  y_ ^ yted2\n",
    "print(' % of points correctly classified : ', ( ( i.shape[0] - np.count_nonzero( i ) )  / i.shape[0] ) * 100 )\n",
    "\n",
    "print(' % of points missclassified  : ', ( np.count_nonzero(( clf.predict( xte3 )- yted2 )) / yted2.shape[0] )* 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  100  and depth =  5  the log loss is :  1.2046900080062566\n",
      "For values of alpha =  100  and depth =  10  the log loss is :  1.2339441021729443\n",
      "For values of alpha =  100  and depth =  20  the log loss is :  1.304013707971388\n",
      "For values of alpha =  200  and depth =  5  the log loss is :  1.1993286315154505\n",
      "For values of alpha =  200  and depth =  10  the log loss is :  1.2193528263625544\n",
      "For values of alpha =  200  and depth =  20  the log loss is :  1.2896243262020486\n",
      "For values of alpha =  500  and depth =  5  the log loss is :  1.1943661211214818\n",
      "For values of alpha =  500  and depth =  10  the log loss is :  1.2168962762804376\n",
      "For values of alpha =  500  and depth =  20  the log loss is :  1.284611574583206\n",
      "For values of alpha =  1000  and depth =  5  the log loss is :  1.1916428318879262\n",
      "For values of alpha =  1000  and depth =  10  the log loss is :  1.2146662457010344\n",
      "For values of alpha =  1000  and depth =  20  the log loss is :  1.286113900207443\n",
      "For values of alpha =  2000  and depth =  5  the log loss is :  1.190565685843375\n",
      "For values of alpha =  2000  and depth =  10  the log loss is :  1.213038713701574\n",
      "For values of alpha =  2000  and depth =  20  the log loss is :  1.2836419161879775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "alpha = [100,200,500,1000,2000] # Various value of Hyperparameter\n",
    "max_depth = [5, 10, 20]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    for j in max_depth:\n",
    "        classifier = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n",
    "        classifier.fit(xtr3, ytrd2)\n",
    "\n",
    "        # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "        # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "        # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "\n",
    "        clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "\n",
    "        # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "        # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "\n",
    "        clf.fit(xtr3, ytrd2)\n",
    "\n",
    "        predictY = clf.predict_proba( xcv3 )\n",
    "\n",
    "        logError.append( log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "        print('For values of alpha = ', i,' and depth = ', j, \" the log loss is : \",log_loss(ycvd2, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for Response Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feaDic( alpha, feature, df ):\n",
    "    \n",
    "    count = xtrd2[ feature ].value_counts()\n",
    "    # output:\n",
    "    #        {BRCA1      174\n",
    "    #         TP53       106\n",
    "    #         EGFR        86.....\n",
    "    \n",
    "    # featDict : Feature Dict, which contains the probability array for each gene/variation\n",
    "    featDict = dict()\n",
    "    \n",
    "    # denominator will contain the number of time that particular feature occured in whole data\n",
    "    \n",
    "    for i, denominator in count.items():\n",
    "        \n",
    "        # vec will contain ( P( yi == 1 / Gi ) probability of gene/variation belongs to particular class\n",
    "        # vec is 9 diamensional vector\n",
    "        vec = []\n",
    "        \n",
    "        c = [1,2,4,5,6,7,893]\n",
    "        \n",
    "        for j in c:\n",
    "           \n",
    "            # print( xtr.loc[ ( xtr['Class'] == 1 ) & ( xtr['Gene'] == 'BRCA1' )] )\n",
    "            #         ID   Gene             Variation  Class  \n",
    "            # 2470  2470  BRCA1                S1715C      1   \n",
    "            # 2486  2486  BRCA1                S1841R      1   \n",
    "            # 2614  2614  BRCA1                   M1R      1....\n",
    "            \n",
    "            # cls_cnt.shape[0] will return the number of rows\n",
    "\n",
    "            cls_cnt = xtrd2.loc[ ( xtrd2['newClass'] == j ) & ( xtrd2[feature] == i ) ]\n",
    "            \n",
    "            # cls_cnt.shape[0] will contain the number of time that particular feature occured in the whole data\n",
    "            \n",
    "            vec.append( ( cls_cnt.shape[0] + alpha * 10 ) / ( denominator + 90 * alpha ) )\n",
    "\n",
    "        # we are adding the gene/variation to the dict as key and vec as value\n",
    "        featDict[i] = vec\n",
    "    return featDict\n",
    "\n",
    "# when we caculate the probability of a feature belongs to any particular class, we apply laplace smoothing\n",
    "# (numerator + 10 *alpha) / (denominator + 90 *alpha ) \n",
    "\n",
    "def feature( alpha, feature, df ):\n",
    "   \n",
    "    featureDict = feaDic( alpha, feature, df )\n",
    "\n",
    "    count = xtrd2[feature].value_counts()\n",
    "    \n",
    "    # feat : Gene_variation feature, it will contain the feature for each feature value in the data\n",
    "    feat = []\n",
    "    \n",
    "    # for every feature values in the given data frame we will check if it is there in the train data then we will add...\n",
    "    #... the feature to gv_fea. if not we will add [1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9] to gv_fea\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        if row[feature] in dict( count ).keys():\n",
    "            feat.append( featureDict[ row[feature] ] )\n",
    "        else:\n",
    "            feat.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n",
    "\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 7)\n",
      "(532, 7)\n",
      "(665, 7)\n",
      "(2124, 7)\n",
      "(532, 7)\n",
      "(665, 7)\n",
      "(2124, 7)\n",
      "(665, 7)\n",
      "(532, 7)\n",
      "(2124, 7)\n",
      "(665, 7)\n",
      "(532, 7)\n"
     ]
    }
   ],
   "source": [
    "# Response-coding of the Gene feature\n",
    "alpha = 1 # alpha is used for laplace smoothing\n",
    "\n",
    "xtrGeneRC1 = np.array( feature( alpha, \"Gene\", xtrd2) )\n",
    "xteGeneRC1 = np.array( feature( alpha, \"Gene\", xted2) )\n",
    "xcvGeneRC1 = np.array( feature( alpha, \"Gene\", xcvd2) )\n",
    "\n",
    "print(xtrGeneRC1.shape )\n",
    "print(xcvGeneRC1.shape )\n",
    "print(xteGeneRC1.shape )\n",
    "\n",
    "\n",
    "# Response-coding of the Variation feature\n",
    "alpha = 1 # alpha is used for laplace smoothing\n",
    "\n",
    "xtrVarRC1 = np.array( feature( alpha, \"Variation\", xtrd2) )\n",
    "xteVarRC1 = np.array( feature( alpha, \"Variation\", xted2) )\n",
    "xcvVarRC1 = np.array( feature( alpha, \"Variation\", xcvd2) )\n",
    "\n",
    "print(xtrVarRC1.shape )\n",
    "print(xcvVarRC1.shape )\n",
    "print(xteVarRC1.shape )\n",
    "\n",
    "\n",
    "xtrT1stRC = np.array( feature( alpha, \"text1st\", xtrd2) )\n",
    "xteT1stRC = np.array( feature( alpha, \"text1st\", xted2) )\n",
    "xcvT1stRC = np.array( feature( alpha, \"text1st\", xcvd2) )\n",
    "\n",
    "print(xtrT1stRC.shape )\n",
    "print(xteT1stRC.shape )\n",
    "print(xcvT1stRC.shape )\n",
    "\n",
    "\n",
    "xtrTLRC = np.array( feature( alpha, \"textLast\", xtrd2) )\n",
    "xteTLRC = np.array( feature( alpha, \"textLast\", xted2) )\n",
    "xcvTLRC = np.array( feature( alpha, \"textLast\", xcvd2) )\n",
    "\n",
    "print(xtrTLRC.shape )\n",
    "print(xteTLRC.shape )\n",
    "print(xcvTLRC.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "# cls_text is a data frame\n",
    "# 1. for every row in data fram consider the 'TEXT'\n",
    "# 2.     Split the words by space\n",
    "# 3.     Make a dict with those words\n",
    "# 4.     Increment its count whenever we see that word\n",
    "\n",
    "def extract(classText):\n",
    "    \n",
    "    dictionary = defaultdict( int )\n",
    "    \n",
    "    for index, row in classText.iterrows():\n",
    "        for word in row['TEXT'].split():\n",
    "            dictionary[word] +=1\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "import math\n",
    "def textResponseC(df):\n",
    "    \n",
    "    textFeat = np.zeros( ( df.shape[0], 7 ) )\n",
    "    \n",
    "    #c = [1,2,4,5,6,7,893]\n",
    "\n",
    "    for i in range(0,7):\n",
    "        row_index = 0\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            prob = 0\n",
    "            for word in row['TEXT'].split():\n",
    "                prob += math.log(( ( dictli[i].get( word, 0 ) + 10 ) / ( totaldict.get( word, 0 ) + 90 ) ))\n",
    "                # here 10 and 90 are alpha fro Laplace smoothing \n",
    "            \n",
    "            textFeat[ row_index ][i] = math.exp( prob / len( row['TEXT'].split() ))\n",
    "            row_index += 1\n",
    "        \n",
    "    return textFeat\n",
    "\n",
    "# Ref : #https://stackoverflow.com/a/1602964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 7)\n",
      "(665, 7)\n",
      "(532, 7)\n"
     ]
    }
   ],
   "source": [
    "dictli = [] # dictli : contains 9 dictoinaries each corresponds to a class\n",
    "\n",
    "c = [1,2,4,5,6,7,893]\n",
    "\n",
    "for i in c:\n",
    "    classText = xtrd2[ xtrd2['newClass'] == i ]  # build a word dict based on the words in that class\n",
    "    dictli.append( extract( classText ) ) # append it to dictli\n",
    "\n",
    "# dictli[i] is build on i'th  class text data\n",
    "\n",
    "totaldict = extract( xtrd2 ) # totaldict is buid on whole training text data\n",
    "# Response coding of text features\n",
    "\n",
    "\n",
    "\n",
    "xtrTextRC1 = textResponseC( xtrd2 )\n",
    "xteTextRC1  = textResponseC( xted2 )\n",
    "xcvTextRC1  = textResponseC( xcvd2 )\n",
    "\n",
    "print( xtrTextRC1.shape ) \n",
    "print( xteTextRC1.shape ) \n",
    "print( xcvTextRC1.shape ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 40)\n",
      "(665, 40)\n",
      "(532, 40)\n"
     ]
    }
   ],
   "source": [
    "# We convert each row values such that they sum to 1  \n",
    "\n",
    "xtrTextRC1 = ( xtrTextRC1.T / xtrTextRC1.sum( axis=1 ) ).T\n",
    "xteTextRC1 = ( xteTextRC1.T / xteTextRC1.sum( axis=1 ) ).T\n",
    "xcvTextRC1 = ( xcvTextRC1.T / xcvTextRC1.sum( axis=1 ) ).T\n",
    "\n",
    "# Ref : # https://stackoverflow.com/a/16202486\n",
    "# giving error on just hstack() and not on np.hstack()\n",
    "\n",
    "xtrRC1 = np.hstack(( xtrGeneRC1, xtrVarRC1, xtrTextRC1, xtrT1stRC, xtrTLRC, xtrCG, xtrCV, xtrLV, xtrLG, xtrLT )) \n",
    "xteRC1 = np.hstack(( xteGeneRC1, xteVarRC1, xteTextRC1, xteT1stRC, xteTLRC, xteCG, xteCV, xteLV, xteLG, xteLT  ))\n",
    "xcvRC1 = np.hstack(( xcvGeneRC1, xcvVarRC1, xcvTextRC1, xcvT1stRC, xcvTLRC, xcvCG, xcvCV, xcvLV, xcvLG, xcvLT  ))  \n",
    "\n",
    "print( xtrRC1.shape )\n",
    "print( xteRC1.shape )\n",
    "print( xcvRC1.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  1e-06  the log loss is :  1.0876691402085665\n",
      "For values of alpha =  1e-05  the log loss is :  1.0863845590336372\n",
      "For values of alpha =  0.0001  the log loss is :  1.0910592198950901\n",
      "For values of alpha =  0.001  the log loss is :  1.1525205784225083\n",
      "For values of alpha =  0.01  the log loss is :  1.2259567350370117\n",
      "For values of alpha =  0.1  the log loss is :  1.241184614097225\n",
      "For values of alpha =  1  the log loss is :  1.2676661626111678\n",
      "For values of alpha =  10  the log loss is :  1.4195499561212752\n",
      "For values of alpha =  100  the log loss is :  1.4903646677522868\n"
     ]
    }
   ],
   "source": [
    "alpha = [ 10 ** x for x in range(-6, 3) ]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    classifier = SGDClassifier(class_weight='balanced', alpha = i, penalty='l2', loss='log', random_state=42)\n",
    "    classifier.fit(xtrRC1, ytrd2)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtrRC1, ytrd2)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcvRC1 )\n",
    "    \n",
    "    logError.append( log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of alpha = ', i, \" the log loss is : \",log_loss(ycvd2, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best alpha :  1e-05\n",
      " The train log loss is :  0.8230172914388513\n",
      " The test log loss is :  1.0725767673466318\n",
      " The cv log loss is :  1.0863845590336372\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "bestA = alpha[ np.argmin(logError) ]\n",
    "clf = SGDClassifier(class_weight='balanced', alpha = bestA, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(xtrRC1, ytrd2)\n",
    "\n",
    "clf = CalibratedClassifierCV( clf, method=\"sigmoid\" )\n",
    "clf.fit(xtrRC1, ytrd2)\n",
    "\n",
    "print(' Best alpha : ', bestA)\n",
    "\n",
    "predictY = clf.predict_proba( xtrRC1 )\n",
    "print(' The train log loss is : ',log_loss( ytrd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xteRC1 )\n",
    "print(' The test log loss is : ',log_loss( yted2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xcvRC1 )\n",
    "print(' The cv log loss is : ',log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of K =  5  the log loss is :  1.051420536259782\n",
      "For values of K =  9  the log loss is :  1.0448571817273438\n",
      "For values of K =  10  the log loss is :  1.0440432095480678\n",
      "For values of K =  11  the log loss is :  1.0475122334099145\n",
      "For values of K =  15  the log loss is :  1.0562538943406925\n",
      "For values of K =  25  the log loss is :  1.0765237454071528\n",
      "For values of K =  31  the log loss is :  1.0804853901498412\n",
      "For values of K =  41  the log loss is :  1.0952513060827043\n",
      "For values of K =  51  the log loss is :  1.101224324090317\n",
      "For values of K =  61  the log loss is :  1.1044569395566426\n",
      "For values of K =  81  the log loss is :  1.1001957342243152\n",
      "For values of K =  91  the log loss is :  1.1073710813799102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = [5, 9, 10, 11, 15, 25, 31, 41, 51, 61, 81, 91] # Various value of Hyperparameter\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in k :\n",
    "    classifier = KNeighborsClassifier( n_neighbors = i ) \n",
    "    classifier.fit(xtrRC1, ytrd2)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtrRC1, ytrd2)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcvRC1 )\n",
    "    \n",
    "    logError.append( log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of K = ', i, \" the log loss is : \",log_loss(ycvd2, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best K :  9\n",
      " The train log loss is :  0.669176825197482\n",
      " The test log loss is :  1.0422140442763765\n",
      " The cv log loss is :  1.0448571817273438\n"
     ]
    }
   ],
   "source": [
    "bestk = 9\n",
    "clf = KNeighborsClassifier( n_neighbors = bestk )\n",
    "clf.fit(xtrRC1, ytrd2)\n",
    "\n",
    "clf = CalibratedClassifierCV( clf, method=\"sigmoid\" )\n",
    "clf.fit(xtrRC1, ytrd2)\n",
    "\n",
    "print(' Best K : ', bestk)\n",
    "\n",
    "predictY = clf.predict_proba( xtrRC1 )\n",
    "print(' The train log loss is : ',log_loss( ytrd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xteRC1 )\n",
    "print(' The test log loss is : ',log_loss( yted2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xcvRC1 )\n",
    "print(' The cv log loss is : ',log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stacking Classifier Response Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier # pip install mlxtend\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression :  Log Loss: 1.15\n",
      "Support vector machines : Log Loss:  1.131183291709275\n",
      "Naive Bayes : Log Loss:  1.237688611965795\n",
      "--------------------------------------------------\n",
      "Stacking Classifer : for the value of alpha: 0.0001  Log Loss: 1.9306765871556373\n",
      "Stacking Classifer : for the value of alpha: 0.001  Log Loss: 1.8186959924057207\n",
      "Stacking Classifer : for the value of alpha: 0.01  Log Loss: 1.4543067126749814\n",
      "Stacking Classifer : for the value of alpha: 0.1  Log Loss: 1.1673756953353236\n",
      "Stacking Classifer : for the value of alpha: 1  Log Loss: 1.100120329499394\n",
      "Stacking Classifer : for the value of alpha: 10  Log Loss: 1.0920669819790216\n",
      "Stacking Classifer : for the value of alpha: 15  Log Loss: 1.0907299119198675\n",
      "Stacking Classifer : for the value of alpha: 20  Log Loss: 1.0901691219500627\n",
      "Stacking Classifer : for the value of alpha: 23  Log Loss: 1.0899903515151579\n",
      "Stacking Classifer : for the value of alpha: 25  Log Loss: 1.0899614461464064\n",
      "Stacking Classifer : for the value of alpha: 30  Log Loss: 1.0899991542259193\n",
      "Stacking Classifer : for the value of alpha: 40  Log Loss: 1.090316722411891\n",
      "Stacking Classifer : for the value of alpha: 50  Log Loss: 1.090940211058856\n",
      "Stacking Classifer : for the value of alpha: 60  Log Loss: 1.0915735915808882\n"
     ]
    }
   ],
   "source": [
    "classifier1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=0)\n",
    "classifier1.fit(xtrRC1, ytrd2)\n",
    "clf1 = CalibratedClassifierCV(classifier1, method=\"sigmoid\")\n",
    "\n",
    "classifier2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\n",
    "classifier2.fit(xtrRC1, ytrd2)\n",
    "clf2 = CalibratedClassifierCV(classifier2, method=\"sigmoid\")\n",
    "\n",
    "\n",
    "classifier3 = MultinomialNB(alpha=0.001)\n",
    "classifier3.fit(xtrRC1, ytrd2)\n",
    "clf3 = CalibratedClassifierCV(classifier3, method=\"sigmoid\")\n",
    "\n",
    "\n",
    "clf1.fit(xtrRC1, ytrd2)\n",
    "print(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(ycvd2, clf1.predict_proba(xcvRC1))))\n",
    "\n",
    "clf2.fit(xtrRC1, ytrd2)\n",
    "print(\"Support vector machines : Log Loss: \", (log_loss(ycvd2, clf2.predict_proba(xcvRC1))))\n",
    "\n",
    "clf3.fit(xtrRC1, ytrd2)\n",
    "print(\"Naive Bayes : Log Loss: \", (log_loss(ycvd2, clf3.predict_proba(xcvRC1))))\n",
    "print(\"-\"*50)\n",
    "\n",
    "alpha = [0.0001,0.001,0.01,0.1,1,10,15, 20, 23, 25, 30, 40,50, 60] \n",
    "best_alpha = 999\n",
    "\n",
    "for i in alpha:\n",
    "    lr = LogisticRegression(C=i)\n",
    "    sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr, use_probas=True)\n",
    "    sclf.fit(xtrRC1, ytrd2)\n",
    "    print(\"Stacking Classifer : for the value of alpha:\",i, \" Log Loss:\",  (log_loss(ycvd2, sclf.predict_proba(xcvRC1))))\n",
    "    log_error = log_loss(ycvd2, sclf.predict_proba(xcvRC1))\n",
    "    if best_alpha > log_error:\n",
    "        best_alpha = log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss (train) on the stacking classifier : 0.9906576802892209\n",
      "Log loss (CV) on the stacking classifier : 1.1673756953353236\n",
      "Log loss (test) on the stacking classifier : 1.1482238640740874\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C = 0.1)\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr, use_probas=True)\n",
    "sclf.fit(xtrRC1, ytrd2)\n",
    "\n",
    "log_error = log_loss(ytrd2, sclf.predict_proba(xtrRC1))\n",
    "print(\"Log loss (train) on the stacking classifier :\",log_error)\n",
    "\n",
    "log_error = log_loss(ycvd2, sclf.predict_proba(xcvRC1))\n",
    "print(\"Log loss (CV) on the stacking classifier :\",log_error)\n",
    "\n",
    "log_error = log_loss(yted2, sclf.predict_proba(xteRC1))\n",
    "print(\"Log loss (test) on the stacking classifier :\",log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stacking Classifier TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression :  Log Loss: 0.98\n",
      "Support vector machines : Log Loss:  1.3258239071326678\n",
      "Naive Bayes : Log Loss:  1.23025640343363\n",
      "--------------------------------------------------\n",
      "Stacking Classifer : for the value of alpha: 0.0001  Log Loss: 1.9307240613365082\n",
      "Stacking Classifer : for the value of alpha: 0.001  Log Loss: 1.814493876840171\n",
      "Stacking Classifer : for the value of alpha: 0.01  Log Loss: 1.3682163310440416\n",
      "Stacking Classifer : for the value of alpha: 0.1  Log Loss: 1.0463442421234976\n",
      "Stacking Classifer : for the value of alpha: 1  Log Loss: 1.153337993577137\n",
      "Stacking Classifer : for the value of alpha: 10  Log Loss: 1.3541705849112775\n",
      "Stacking Classifer : for the value of alpha: 100  Log Loss: 1.5204248277559287\n",
      "Stacking Classifer : for the value of alpha: 1000  Log Loss: 1.6226971824492062\n"
     ]
    }
   ],
   "source": [
    "classifier1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=0)\n",
    "classifier1.fit(xtr3, ytrd2)\n",
    "clf1 = CalibratedClassifierCV(classifier1, method=\"sigmoid\")\n",
    "\n",
    "classifier2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\n",
    "classifier2.fit(xtr3, ytrd2)\n",
    "clf2 = CalibratedClassifierCV(classifier2, method=\"sigmoid\")\n",
    "\n",
    "\n",
    "classifier3 = MultinomialNB(alpha=0.001)\n",
    "classifier3.fit(xtr3, ytrd2)\n",
    "clf3 = CalibratedClassifierCV(classifier3, method=\"sigmoid\")\n",
    "\n",
    "\n",
    "clf1.fit(xtr3, ytrd2)\n",
    "print(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(ycvd2, clf1.predict_proba(xcv3))))\n",
    "\n",
    "clf2.fit(xtr3, ytrd2)\n",
    "print(\"Support vector machines : Log Loss: \", (log_loss(ycvd2, clf2.predict_proba(xcv3))))\n",
    "\n",
    "clf3.fit(xtr3, ytrd2)\n",
    "print(\"Naive Bayes : Log Loss: \", (log_loss(ycvd2, clf3.predict_proba(xcv3))))\n",
    "print(\"-\"*50)\n",
    "\n",
    "alpha = [0.0001,0.001,0.01,0.1,1,10, 100, 1000] \n",
    "best_alpha = 999\n",
    "\n",
    "for i in alpha:\n",
    "    lr = LogisticRegression( C = i )\n",
    "    sclf = StackingClassifier( classifiers = [clf1, clf2, clf3], meta_classifier = lr, use_probas=True)\n",
    "    sclf.fit(xtr3, ytrd2)\n",
    "    print(\"Stacking Classifer : for the value of alpha:\",i, \" Log Loss:\",  (log_loss(ycvd2, sclf.predict_proba(xcv3))))\n",
    "    log_error = log_loss(ycvd2, sclf.predict_proba(xcv3))\n",
    "    if best_alpha > log_error:\n",
    "        best_alpha = log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss (train) on the stacking classifier : 0.5665342644775587\n",
      "Log loss (CV) on the stacking classifier : 1.0666129160151658\n",
      "Log loss (test) on the stacking classifier : 1.0343899497657905\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C = 0.1)\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr, use_probas=True)\n",
    "sclf.fit(xtr3, ytrd2)\n",
    "\n",
    "log_error = log_loss(ytrd2, sclf.predict_proba(xtr3))\n",
    "print(\"Log loss (train) on the stacking classifier :\",log_error)\n",
    "\n",
    "log_error = log_loss(ycvd2, sclf.predict_proba(xcv3))\n",
    "print(\"Log loss (CV) on the stacking classifier :\",log_error)\n",
    "\n",
    "log_error = log_loss(yted2, sclf.predict_proba(xte3))\n",
    "print(\"Log loss (test) on the stacking classifier :\",log_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## XG Boost TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  0.01  and depth =  1  the log loss is :  1.2189406311642224\n",
      "For values of alpha =  0.01  and depth =  5  the log loss is :  1.2064912828417294\n",
      "For values of alpha =  0.01  and depth =  10  the log loss is :  1.2158805796116527\n",
      "For values of alpha =  0.01  and depth =  50  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0.01  and depth =  100  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0.01  and depth =  500  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0.01  and depth =  600  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0.1  and depth =  1  the log loss is :  1.2189406311642224\n",
      "For values of alpha =  0.1  and depth =  5  the log loss is :  1.2064912828417294\n",
      "For values of alpha =  0.1  and depth =  10  the log loss is :  1.2158805796116527\n",
      "For values of alpha =  0.1  and depth =  50  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0.1  and depth =  100  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0.1  and depth =  500  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0.1  and depth =  600  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0  and depth =  1  the log loss is :  1.2189406311642224\n",
      "For values of alpha =  0  and depth =  5  the log loss is :  1.2064912828417294\n",
      "For values of alpha =  0  and depth =  10  the log loss is :  1.2158805796116527\n",
      "For values of alpha =  0  and depth =  50  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0  and depth =  100  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0  and depth =  500  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  0  and depth =  600  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  1  and depth =  1  the log loss is :  1.2189406311642224\n",
      "For values of alpha =  1  and depth =  5  the log loss is :  1.2064912828417294\n",
      "For values of alpha =  1  and depth =  10  the log loss is :  1.2158805796116527\n",
      "For values of alpha =  1  and depth =  50  the log loss is :  1.2269509580886535\n",
      "For values of alpha =  1  and depth =  100  the log loss is :  1.2269509580886535\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "max_depth = [ 1, 5, 10, 50, 100, 500, 600 ]\n",
    "learning_rate = [ 0.01, 0.1, 0 , 1  ]\n",
    "\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in learning_rate :\n",
    "    for j in max_depth:\n",
    "        \n",
    "        classifier = xgb.XGBClassifier( objective = \"binary:logistic\", random_state = 42, max_depth = j, eta = i )\n",
    "        classifier.fit(xtr3, ytrd2)\n",
    "\n",
    "        # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "        # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "        # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "\n",
    "        clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "\n",
    "        # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "        # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "\n",
    "        clf.fit(xtr3, ytrd2)\n",
    "\n",
    "        predictY = clf.predict_proba( xcv3 )\n",
    "\n",
    "        logError.append( log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "        print('For values of alpha = ', i,' and depth = ', j, \" the log loss is : \",log_loss(ycvd2, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creating Avg Word 2 vec features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function Creating a list that contains all the words in the data column\n",
    "\n",
    "def stow( data ):\n",
    "    sentence = []\n",
    "\n",
    "    for sen in tqdm( data.values ):\n",
    "        fsentence = []\n",
    "    \n",
    "        for w in sen.split():\n",
    "            for cw in w.split():\n",
    "            \n",
    "                if cw.isalpha():\n",
    "                    fsentence.append( cw.lower() )\n",
    "                else:\n",
    "                    continue\n",
    "        sentence.append( fsentence )\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2124/2124 [00:06<00:00, 329.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# For Train\n",
    "\n",
    "xtrw = stow( xtrd2['TEXT'] ) # List of words for xtrain\n",
    "\n",
    "print( len( xtrw ) )\n",
    "\n",
    "xtrmodel = gensim.models.Word2Vec( xtrw, min_count = 5, size = 100, workers = 4 )\n",
    "# min_count : If a word dosent apper more then the value assigned do't construct w2v for it\n",
    "# size: dimension of vector to be constructed  ( 100 - 300 most cases )\n",
    "# workers: If you hav a multi cores you can set it to 4 for performence\n",
    "\n",
    "xtrwords = list( xtrmodel.wv.vocab )\n",
    "len( xtrwords )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 532/532 [00:01<00:00, 337.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17063"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Cross validation\n",
    "\n",
    "xcvw = stow( xcvd2['TEXT'] )\n",
    "\n",
    "print( len( xcvw ) )\n",
    "\n",
    "import gensim\n",
    "\n",
    "xcvmodel = gensim.models.Word2Vec( xcvw, min_count = 5, size =100, workers = 4 )\n",
    "\n",
    "xcvwords = list( xcvmodel.wv.vocab )\n",
    "len( xcvwords )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 665/665 [00:02<00:00, 270.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19067"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Test\n",
    "\n",
    "xtew = stow( xted2['TEXT'] )\n",
    "\n",
    "print( len( xtew ) )\n",
    "\n",
    "import gensim\n",
    "\n",
    "xtemodel = gensim.models.Word2Vec( xtew, min_count = 5, size =100, workers = 4 )\n",
    "\n",
    "xtewords = list( xtemodel.wv.vocab )\n",
    "len( xtewords )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg Word 2 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def avgw2v( data, words, model ):\n",
    "    sentV = [] # average word 2 vec for each essay is stored in this\n",
    "\n",
    "    for sent in tqdm( data ):\n",
    "    \n",
    "        svec = np.zeros(100)  # creates a np array with 50 0's we took 50 here cause we took 50 as our size in Word2Vec model\n",
    "        cnw = 0\n",
    "    \n",
    "        for w in sent:\n",
    "            if w in words:\n",
    "                vec = model.wv[ w ]  # Computing word 2 vec\n",
    "                svec += vec     # Add it to the svec\n",
    "                cnw += 1\n",
    "            \n",
    "        if cnw != 0:        \n",
    "            svec /= cnw  # Averaging with the count of number of words with valid vector in the Essay\n",
    "        sentV.append( svec )\n",
    "    \n",
    "    return sentV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2124/2124 [17:37<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrAW2V = np.asarray( avgw2v( xtrw, xtrwords, xtrmodel ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 532/532 [03:05<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "xcvAW2V = np.asarray( avgw2v( xcvw, xcvwords, xcvmodel ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 665/665 [03:58<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "xteAW2V = np.asarray( avgw2v( xtew, xtewords, xtemodel ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 2383)\n",
      "(665, 2383)\n",
      "(532, 2383)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "xtr4 = hstack(( xtrGeneTF2, xtrVarTF2, xtrAW2V, xtrT1stTF2, xtrTLTF2, xtrCG, xtrCV, xtrLV, xtrLG, xtrLT )).tocsr()\n",
    "xte4 = hstack(( xteGeneTF2, xteVarTF2, xteAW2V, xteT1stTF2, xteTLTF2, xteCG, xteCV, xteLV, xteLG, xteLT )).tocsr()\n",
    "xcv4 = hstack(( xcvGeneTF2, xcvVarTF2, xcvAW2V, xcvT1stTF2, xcvTLTF2, xcvCG, xcvCV, xcvLV, xcvLG, xcvLT )).tocsr()\n",
    "\n",
    "print( xtr4.shape )\n",
    "print( xte4.shape )\n",
    "print( xcv4.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  1e-06  the log loss is :  1.3895227228960962\n",
      "For values of alpha =  1e-05  the log loss is :  1.4004286315053942\n",
      "For values of alpha =  0.0001  the log loss is :  1.3625960910742605\n",
      "For values of alpha =  0.001  the log loss is :  1.3280667553562957\n",
      "For values of alpha =  0.01  the log loss is :  1.5540800684701759\n",
      "For values of alpha =  0.1  the log loss is :  1.6740785605882313\n",
      "For values of alpha =  1  the log loss is :  1.818205282356213\n",
      "For values of alpha =  10  the log loss is :  1.8270087034134774\n",
      "For values of alpha =  100  the log loss is :  1.83782563604614\n"
     ]
    }
   ],
   "source": [
    "alpha = [ 10 ** x for x in range(-6, 3) ]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    classifier = SGDClassifier( alpha = i, penalty='l2', loss='log', random_state=42)\n",
    "    classifier.fit(xtr4, ytrd2)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtr4, ytrd2)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcv4 )\n",
    "    \n",
    "    logError.append( log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of alpha = ', i, \" the log loss is : \",log_loss(ycvd2, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best alpha :  0.001\n",
      " The train log loss is :  0.733611012931541\n",
      " The test log loss is :  1.3423818313732172\n",
      " The cv log loss is :  1.3280667553562957\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "bestA = alpha[ np.argmin(logError) ]\n",
    "clf = SGDClassifier( alpha = bestA, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(xtr4, ytrd2)\n",
    "\n",
    "clf = CalibratedClassifierCV( clf, method=\"sigmoid\" )\n",
    "clf.fit(xtr4, ytrd2)\n",
    "\n",
    "print(' Best alpha : ', bestA)\n",
    "\n",
    "predictY = clf.predict_proba( xtr4 )\n",
    "print(' The train log loss is : ',log_loss( ytrd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xte4 )\n",
    "print(' The test log loss is : ',log_loss( yted2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "\n",
    "predictY = clf.predict_proba( xcv4 )\n",
    "print(' The cv log loss is : ',log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  100  and depth =  5  the log loss is :  1.6399353144035769\n",
      "For values of alpha =  100  and depth =  10  the log loss is :  1.707837323070964\n",
      "For values of alpha =  100  and depth =  20  the log loss is :  1.7206524460424888\n",
      "For values of alpha =  200  and depth =  5  the log loss is :  1.642047641854365\n",
      "For values of alpha =  200  and depth =  10  the log loss is :  1.6923733735249114\n",
      "For values of alpha =  200  and depth =  20  the log loss is :  1.7138324387673893\n",
      "For values of alpha =  500  and depth =  5  the log loss is :  1.6348697728478907\n",
      "For values of alpha =  500  and depth =  10  the log loss is :  1.6746941057367744\n",
      "For values of alpha =  500  and depth =  20  the log loss is :  1.7005156895609859\n",
      "For values of alpha =  1000  and depth =  5  the log loss is :  1.6349406963573498\n",
      "For values of alpha =  1000  and depth =  10  the log loss is :  1.668381162690545\n",
      "For values of alpha =  1000  and depth =  20  the log loss is :  1.699485386582567\n",
      "For values of alpha =  2000  and depth =  5  the log loss is :  1.627884391847224\n",
      "For values of alpha =  2000  and depth =  10  the log loss is :  1.6636670353603025\n",
      "For values of alpha =  2000  and depth =  20  the log loss is :  1.696684060420251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "alpha = [100,200,500,1000,2000] # Various value of Hyperparameter\n",
    "max_depth = [5, 10, 20]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    for j in max_depth:\n",
    "        classifier = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n",
    "        classifier.fit(xtr4, ytrd2)\n",
    "\n",
    "        # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "        # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "        # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "\n",
    "        clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "\n",
    "        # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "        # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "\n",
    "        clf.fit(xtr4, ytrd2)\n",
    "\n",
    "        predictY = clf.predict_proba( xcv4 )\n",
    "\n",
    "        logError.append( log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "        print('For values of alpha = ', i,' and depth = ', j, \" the log loss is : \",log_loss(ycvd2, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 k best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2124, 2000)\n",
      "(532, 2000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(665, 2000)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Select features according to the k highest scores.\n",
    "\n",
    "topFeatures = SelectKBest( score_func = chi2, k = 2000 )\n",
    "# score_func : Score func which is applied to a pair of features X and label Y, It returns an array of scores 1 for each feature\n",
    "# then SelectKBest simply retains the first k features of dataset X with the highest scores.\n",
    "# k = no. of top features to be retained\n",
    "# chi2 : compute the chi2 statistic between each feature of X and Y a small val means the feature is independant of Y \n",
    "\n",
    "xtr3Top2k = topFeatures.fit_transform( xtr3, ytrd2 )  \n",
    "\n",
    "print( xtr3Top2k.shape )\n",
    "\n",
    "xcv3Top2k = topFeatures.fit_transform( xcv3, ycvd2 )\n",
    "print( xcv3Top2k.shape )\n",
    "\n",
    "xte3Top2k = topFeatures.fit_transform( xte3, yted2 )\n",
    "\n",
    "xte3Top2k.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression  Avg Word 2 vec Top 2k features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  1e-06  the log loss is :  2.6548942426171296\n",
      "For values of alpha =  1e-05  the log loss is :  2.8707292003333023\n",
      "For values of alpha =  0.0001  the log loss is :  3.0217231044237765\n",
      "For values of alpha =  0.001  the log loss is :  2.3684101836368794\n",
      "For values of alpha =  0.01  the log loss is :  1.9762332980382251\n",
      "For values of alpha =  0.1  the log loss is :  1.881230647041772\n",
      "For values of alpha =  1  the log loss is :  1.8684391226195518\n",
      "For values of alpha =  10  the log loss is :  1.8661773824737868\n",
      "For values of alpha =  100  the log loss is :  1.8659120157274738\n"
     ]
    }
   ],
   "source": [
    "alpha = [ 10 ** x for x in range(-6, 3) ]\n",
    "\n",
    "logError = [ ]\n",
    "\n",
    "for i in alpha :\n",
    "    classifier = SGDClassifier( alpha = i, penalty='l2', loss='log', random_state=42)\n",
    "    classifier.fit(xtr3Top2k, ytrd2)\n",
    "    \n",
    "    # ClaibratedClassifierCV() : Probability calibration with isotonic regression or sigmoid.\n",
    "    # With this class, the base_estimator is fit on the train set of the cross-validation generator and the ......\n",
    "    # .... test set is used for calibration. The probabilities for each of the folds are then averaged for prediction\n",
    "    \n",
    "    clf = CalibratedClassifierCV( classifier, method=\"sigmoid\" )\n",
    "    \n",
    "    # base_estimator : (classifier) The classifier whose output decision func needs to be calibrated to offer more accurate...\n",
    "    # .... predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.\n",
    "    \n",
    "    clf.fit(xtr3Top2k, ytrd2)\n",
    "    \n",
    "    predictY = clf.predict_proba( xcv3Top2k )\n",
    "    \n",
    "    logError.append( log_loss( ycvd2, predictY, labels = clf.classes_, eps = 1e-15) )\n",
    "    print('For values of alpha = ', i, \" the log loss is : \",log_loss(ycvd2, predictY, labels=clf.classes_, eps=1e-15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t Models and their Corresponding log loss with differnt featurization \n",
      " \n",
      "+----------------------------------------+------------------+------------------+---------------+-----------------+\n",
      "|                 Model                  |  Featurization   |  Train log loss  |  CV log loss  |  Test log loss  |\n",
      "+----------------------------------------+------------------+------------------+---------------+-----------------+\n",
      "|          Logistic Regression           |      TFIDF       |      0.5302      |     0.9768    |      0.9257     |\n",
      "|      Linear SVM (balanced class)       |      TFIDF       |      0.5760      |     0.9946    |      0.9432     |\n",
      "|             Random Forest              |      TFIDF       |        *         |     1.1905    |        *        |\n",
      "|    Stacking CLassifier (LR SVM NB)     |      TFIDF       |      0.5665      |     1.0666    |      1.0343     |\n",
      "|                XG Boost                |      TFIDF       |        *         |     1.2064    |        *        |\n",
      "|                                        |                  |                  |               |                 |\n",
      "|  Logistic Regression (balanced class)  |  ResponseCoding  |      0.8230      |     1.0863    |      1.0725     |\n",
      "|          K Nearest Neighbour           |  ResponseCoding  |      0.6691      |     1.0448    |      1.0422     |\n",
      "|             Random Forest              |  ResponseCoding  |        *         |     1.3279    |        *        |\n",
      "|    Stacking CLassifier (LR SVM NB)     |  ResponseCoding  |      0.9906      |     1.1673    |      1.1482     |\n",
      "|                                        |                  |                  |               |                 |\n",
      "|          Logistic Regression           |       AW2V       |        *         |     1.3280    |        *        |\n",
      "|             Random Forest              |       AW2V       |        *         |     1.6278    |        *        |\n",
      "|     Logistic Regression Top2K feat     |       AW2V       |        *         |     1.8659    |        *        |\n",
      "+----------------------------------------+------------------+------------------+---------------+-----------------+\n",
      "\n",
      "\t\t\t * : ignored calculation since CV log loss > 1.19 \n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "    \n",
    "x = PrettyTable()\n",
    "   \n",
    "x.field_names = [ \" Model \", \" Featurization \", \" Train log loss \", \" CV log loss \", \" Test log loss \" ]\n",
    "\n",
    "x.add_row( [ \" Logistic Regression \", \" TFIDF \", \" 0.5302 \", \" 0.9768 \", \" 0.9257 \" ] )\n",
    "x.add_row( [ \" Linear SVM (balanced class) \", \" TFIDF \", \" 0.5760 \", \" 0.9946 \", \" 0.9432 \" ] )\n",
    "#x.add_row( [ \" K Nearest Neighbour \", \" TFIDF \", \" * \", \" 1.2574 \", \" * \" ] )\n",
    "x.add_row( [ \" Random Forest \", \" TFIDF \", \" * \", \" 1.1905 \", \" * \" ] )\n",
    "x.add_row( [ \" Stacking CLassifier (LR SVM NB) \", \" TFIDF \", \" 0.5665 \", \" 1.0666 \", \" 1.0343 \" ] )\n",
    "x.add_row( [ \" XG Boost \", \" TFIDF \", \" * \", \" 1.2064 \", \" * \" ] )\n",
    "x.add_row([\"\",\"\",\"\",\"\",\"\"])\n",
    "\n",
    "x.add_row( [ \" Logistic Regression (balanced class) \", \" ResponseCoding \", \" 0.8230 \", \" 1.0863 \", \" 1.0725 \" ] )\n",
    "x.add_row( [ \" K Nearest Neighbour \", \" ResponseCoding \", \" 0.6691 \", \" 1.0448 \", \"  1.0422 \" ] )\n",
    "x.add_row( [ \" Random Forest \", \" ResponseCoding \", \" * \", \" 1.3279 \", \" * \" ] )\n",
    "x.add_row( [ \" Stacking CLassifier (LR SVM NB) \", \" ResponseCoding \", \" 0.9906 \", \" 1.1673 \", \" 1.1482 \" ] )\n",
    "x.add_row([\"\",\"\",\"\",\"\",\"\"])\n",
    "\n",
    "x.add_row( [ \" Logistic Regression \", \" AW2V \", \" * \", \" 1.3280 \", \" * \" ] )\n",
    "x.add_row( [ \" Random Forest \", \" AW2V \", \" * \", \" 1.6278 \", \" * \" ] )\n",
    "x.add_row( [ \" Logistic Regression Top2K feat \", \" AW2V \", \" * \", \" 1.8659 \", \" * \" ] )\n",
    "\n",
    "print(\"\\n\\t\\t\\t Models and their Corresponding log loss with differnt featurization \\n \")\n",
    "print( x )\n",
    "\n",
    "print(\"\\n\\t\\t\\t * : ignored calculation since CV log loss > 1.19 \")\n",
    "# Ref : http://zetcode.com/python/prettytable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3> <br> From the above table we can see that the best Model after Feature Engineering ( combining Class + creating new Features ) is : <br><br>\n",
    "Logistic Regression with TFIDF text featurization with Test log loss of 0.9257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
